[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "devansh’s blog",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nSegmenting Carvana using Resnet-18\n\n\n\n\n\n\ncode\n\n\nimage segmentation\n\n\n\nThis post implements image segmentation on the Carvana dataset using a pre-trained ResNet18 model adapted for segmentation tasks.\n\n\n\n\n\nDec 31, 2024\n\n\nDevansh Lodha\n\n\n\n\n\n\n\n\n\n\n\n\nImage Reconstruction using Matrix Factorization\n\n\n\n\n\n\ncode\n\n\nimage processing\n\n\n\nThis post will explore image reconstruction using matrix factorization with Spongebob!\n\n\n\n\n\nDec 28, 2024\n\n\nDevansh Lodha\n\n\n\n\n\n\n\n\n\n\n\n\nHistogram Equalization and Matching\n\n\n\n\n\n\ncode\n\n\nprobability theory\n\n\nimage processing\n\n\n\nThis post explores histogram matching and equalization between images of Elon Musk, Lenna, a Panda and the IIT Gandhinagar campus.\n\n\n\n\n\nDec 2, 2024\n\n\nDevansh Lodha\n\n\n\n\n\n\n\n\n\n\n\n\nOtsu’s Threshold Algorithm\n\n\n\n\n\n\ncode\n\n\nprobability theory\n\n\nimage processing\n\n\n\nWe’ll binarize images of Tom and Jerry, Pink Panther and some book pages. Then, we’ll challenge Otsu’s method with noisy inputs.\n\n\n\n\n\nDec 2, 2024\n\n\nDevansh Lodha\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html",
    "href": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html",
    "title": "Segmenting Carvana using Resnet-18",
    "section": "",
    "text": "Code\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch.nn.functional as F\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#setup-and-imports",
    "href": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#setup-and-imports",
    "title": "Segmenting Carvana using Resnet-18",
    "section": "",
    "text": "Code\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom PIL import Image\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nfrom tqdm import tqdm\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\nimport torch.nn.functional as F\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'"
  },
  {
    "objectID": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#dataset-exploration",
    "href": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#dataset-exploration",
    "title": "Segmenting Carvana using Resnet-18",
    "section": "Dataset Exploration",
    "text": "Dataset Exploration\n\n\nCode\nclass CarvanaDataset(Dataset):\n    def __init__(self, image_dir, mask_dir, transform=None):\n        self.image_dir = image_dir\n        self.mask_dir = mask_dir\n        self.transform = transform\n        self.images = os.listdir(image_dir)\n    \n    def __len__(self):\n        return len(self.images)\n    \n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.images[idx])\n        mask_path = os.path.join(self.mask_dir, self.images[idx].replace('.jpg', '_mask.gif'))\n        image = np.array(Image.open(img_path).convert('RGB'))\n        mask = np.array(Image.open(mask_path).convert('L'), dtype=np.float32)\n        mask[mask == 255.0] = 1.0\n        \n        if self.transform:\n            augmentations = self.transform(image=image, mask=mask)\n            image = augmentations['image']\n            mask = augmentations['mask']\n        \n        return image, mask\n\n\n\n\nCode\ndef visualize_dataset_samples(dataset, num_samples=3):\n    plt.figure(figsize=(15, 5*num_samples))\n    for i in range(num_samples):\n        image, mask = dataset[i]\n        \n        # Convert tensors back to numpy arrays for plotting\n        if torch.is_tensor(image):\n            image = image.numpy().transpose(1, 2, 0)\n            # Denormalize if needed\n            image = (image * [1.0, 1.0, 1.0]) + [0.0, 0.0, 0.0]\n            image = np.clip(image, 0, 1)\n        \n        plt.subplot(num_samples, 3, i*3 + 1)\n        plt.imshow(image)\n        plt.title(f'Image {i}')\n        plt.axis('off')\n        \n        plt.subplot(num_samples, 3, i*3 + 2)\n        plt.imshow(mask, cmap='gray')\n        plt.title(f'Mask {i}')\n        plt.axis('off')\n        \n        plt.subplot(num_samples, 3, i*3 + 3)\n        # Overlay mask on image\n        overlay = image.copy()\n        overlay[mask == 1] = [1, 0, 0]  # Red overlay for segmentation\n        plt.imshow(overlay)\n        plt.title(f'Overlay {i}')\n        plt.axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\ndataset = CarvanaDataset('data/train', 'data/train_masks', transform=None)\n\n\n\n\nCode\nvisualize_dataset_samples(dataset)"
  },
  {
    "objectID": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#model-architecture",
    "href": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#model-architecture",
    "title": "Segmenting Carvana using Resnet-18",
    "section": "3. Model Architecture",
    "text": "3. Model Architecture\n\n\nCode\nclass ResNetSegmentation(nn.Module):\n    def __init__(self, num_classes=1):\n        super(ResNetSegmentation, self).__init__()\n        # Load pretrained ResNet18\n        resnet = models.resnet18(pretrained=True)\n        \n        # Encoder (use ResNet layers)\n        self.encoder1 = nn.Sequential(\n            resnet.conv1,\n            resnet.bn1,\n            resnet.relu\n        )  # 64 channels\n        self.pool = resnet.maxpool\n        self.encoder2 = resnet.layer1  # 64 channels\n        self.encoder3 = resnet.layer2  # 128 channels\n        self.encoder4 = resnet.layer3  # 256 channels\n        self.encoder5 = resnet.layer4  # 512 channels\n        \n        # Decoder (like upstream of UNET)\n        self.decoder1 = nn.Sequential(\n            nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2),\n            nn.BatchNorm2d(256),\n            nn.ReLU(inplace=True)\n        )\n        self.decoder2 = nn.Sequential(\n            nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2),\n            nn.BatchNorm2d(128),\n            nn.ReLU(inplace=True)\n        )\n        self.decoder3 = nn.Sequential(\n            nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(inplace=True)\n        )\n        self.decoder4 = nn.Sequential(\n            nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2),\n            nn.BatchNorm2d(32),\n            nn.ReLU(inplace=True)\n        )\n        self.decoder5 = nn.Sequential(\n            nn.ConvTranspose2d(32, num_classes, kernel_size=2, stride=2),\n        )\n        \n    def forward(self, x):\n        # Store input size for later resizing\n        input_size = x.shape[-2:]\n        \n        # Encode\n        x1 = self.encoder1(x)\n        x1p = self.pool(x1)\n        x2 = self.encoder2(x1p)\n        x3 = self.encoder3(x2)\n        x4 = self.encoder4(x3)\n        x5 = self.encoder5(x4)\n        \n        # Decode\n        d1 = self.decoder1(x5)\n        d2 = self.decoder2(d1)\n        d3 = self.decoder3(d2)\n        d4 = self.decoder4(d3)\n        d5 = self.decoder5(d4)\n        \n        # Resize output to match target size\n        if d5.shape[-2:] != input_size:\n            d5 = F.interpolate(d5, size=input_size, mode='bilinear', align_corners=False)\n        \n        return d5"
  },
  {
    "objectID": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#training-setup",
    "href": "posts/carvana_seg_resnet18/carvana_seg_resnet18.html#training-setup",
    "title": "Segmenting Carvana using Resnet-18",
    "section": "4. Training Setup",
    "text": "4. Training Setup\n\n\nCode\n# Hyperparameters\nLEARNING_RATE = 1e-4\nDEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\nBATCH_SIZE = 8\nNUM_EPOCHS = 5\n\n# Directories\nTRAIN_IMG_DIR = \"data/train/\"\nTRAIN_MASK_DIR = \"data/train_masks/\"\nVAL_IMG_DIR = \"data/val/\"\nVAL_MASK_DIR = \"data/val_masks/\"\n\n# Transforms\ntrain_transform = A.Compose([\n    A.Resize(1280, 1918),  # Force specific dimensions\n    A.Rotate(limit=35, p=0.5),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.1),\n    A.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n        max_pixel_value=255.0\n    ),\n    ToTensorV2()\n])\n\nval_transform = A.Compose([\n    A.Resize(1280, 1918),  # Force specific dimensions\n    A.Normalize(\n        mean=[0.485, 0.456, 0.406],\n        std=[0.229, 0.224, 0.225],\n        max_pixel_value=255.0\n    ),\n    ToTensorV2()\n])\n\ndef train_one_epoch(loader, model, optimizer, loss_fn, device):\n    model.train()\n    loop = tqdm(loader)\n    running_loss = 0.0\n    \n    for batch_idx, (data, targets) in enumerate(loop):\n        data = data.to(device)\n        targets = targets.float().unsqueeze(1).to(device)\n        \n        # forward\n        predictions = model(data)\n        loss = loss_fn(predictions, targets)\n        \n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        running_loss += loss.item()\n        loop.set_postfix(loss=loss.item())\n    \n    return running_loss / len(loader)\n\ndef evaluate(loader, model, device):\n    model.eval()\n    dice_score = 0\n    num_correct = 0\n    num_pixels = 0\n    \n    with torch.no_grad():\n        for x, y in loader:\n            x = x.to(device)\n            y = y.to(device).unsqueeze(1)\n            preds = torch.sigmoid(model(x))\n            preds = (preds &gt; 0.5).float()\n            \n            num_correct += (preds == y).sum()\n            num_pixels += torch.numel(preds)\n            dice_score += (2 * (preds * y).sum()) / ((preds + y).sum() + 1e-8)\n    \n    accuracy = (num_correct / num_pixels * 100).item()\n    dice = (dice_score / len(loader)).item()\n    \n    return accuracy, dice\n\ndef plot_predictions(model, val_loader, device, num_samples=3):\n    model.eval()\n    fig, axes = plt.subplots(num_samples, 3, figsize=(15, 5*num_samples))\n    \n    with torch.no_grad():\n        for idx, (x, y) in enumerate(val_loader):\n            if idx &gt;= num_samples:\n                break\n                \n            x = x.to(device)\n            preds = torch.sigmoid(model(x))\n            preds = (preds &gt; 0.5).float()\n            \n            # Convert to numpy and denormalize\n            img = x[0].cpu().numpy().transpose(1, 2, 0)\n            img = img * [0.229, 0.224, 0.225] + [0.485, 0.456, 0.406]\n            img = np.clip(img * 255, 0, 255).astype(np.uint8)\n            \n            mask = y[0].cpu().numpy() * 255\n            pred = preds[0].cpu().numpy()[0] * 255\n            \n            axes[idx, 0].imshow(img)\n            axes[idx, 0].set_title('Original Image')\n            axes[idx, 0].axis('off')\n            \n            axes[idx, 1].imshow(mask, cmap='gray')\n            axes[idx, 1].set_title('Ground Truth')\n            axes[idx, 1].axis('off')\n            \n            axes[idx, 2].imshow(pred, cmap='gray')\n            axes[idx, 2].set_title('Prediction')\n            axes[idx, 2].axis('off')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef plot_training_history(train_losses, val_accuracies, val_dices):\n    epochs = range(1, len(train_losses) + 1)\n\n    plt.figure(figsize=(15, 5))\n    \n    plt.subplot(1, 3, 1)\n    plt.plot(epochs, train_losses, 'b-')\n    plt.title('Training Loss')\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    \n    plt.subplot(1, 3, 2)\n    plt.plot(epochs, val_accuracies, 'g-')\n    plt.title('Validation Accuracy')\n    plt.xlabel('Epoch')\n    plt.ylabel('Accuracy (%)')\n    \n    plt.subplot(1, 3, 3)\n    plt.plot(epochs, val_dices, 'r-')\n    plt.title('Validation Dice Score')\n    plt.xlabel('Epoch')\n    plt.ylabel('Dice Score')\n    \n    plt.tight_layout()\n    plt.show()\n\ndef main():\n    # Create data loaders\n    train_ds = CarvanaDataset(TRAIN_IMG_DIR, TRAIN_MASK_DIR, train_transform)\n    val_ds = CarvanaDataset(VAL_IMG_DIR, VAL_MASK_DIR, val_transform)\n    train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True)\n    val_loader = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False)\n    \n    # Initialize model, loss, optimizer\n    model = ResNetSegmentation().to(DEVICE)\n    loss_fn = nn.BCEWithLogitsLoss()\n    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n    \n    # Training history\n    train_losses = []\n    val_accuracies = []\n    val_dices = []\n    \n    # Training loop\n    print(\"Starting training...\")\n    for epoch in range(NUM_EPOCHS):\n        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n        \n        # Train\n        train_loss = train_one_epoch(train_loader, model, optimizer, loss_fn, DEVICE)\n        train_losses.append(train_loss)\n        \n        # Evaluate\n        accuracy, dice = evaluate(val_loader, model, DEVICE)\n        val_accuracies.append(accuracy)\n        val_dices.append(dice)\n        \n        print(f\"Train Loss: {train_loss:.4f}\")\n        print(f\"Val Accuracy: {accuracy:.2f}%\")\n        print(f\"Val Dice Score: {dice:.4f}\")\n        \n        # Plot some predictions at the end of each epoch\n        plot_predictions(model, val_loader, DEVICE)\n    \n    # Plot training history\n    plot_training_history(train_losses, val_accuracies, val_dices)\n    \n    # Save final model\n    torch.save({\n        'model_state_dict': model.state_dict(),\n        'optimizer_state_dict': optimizer.state_dict(),\n        'train_losses': train_losses,\n        'val_accuracies': val_accuracies,\n        'val_dices': val_dices\n    }, 'resnet18_segmentation_final.pth')\n\n\n\n\nCode\nmain()\n\n\nStarting training...\nEpoch 1/5\n\n\n100%|██████████| 630/630 [26:27&lt;00:00,  2.52s/it, loss=0.147]\n\n\nTrain Loss: 0.2297\nVal Accuracy: 99.72%\nVal Dice Score: 0.9930\n\n\n\n\n\n\n\n\n\nEpoch 2/5\n\n\n100%|██████████| 630/630 [23:35&lt;00:00,  2.25s/it, loss=0.0688]\n\n\nTrain Loss: 0.0985\nVal Accuracy: 99.79%\nVal Dice Score: 0.9948\n\n\n\n\n\n\n\n\n\nEpoch 3/5\n\n\n100%|██████████| 630/630 [25:59&lt;00:00,  2.48s/it, loss=0.0394]\n\n\nTrain Loss: 0.0540\nVal Accuracy: 99.81%\nVal Dice Score: 0.9953\n\n\n\n\n\n\n\n\n\nEpoch 4/5\n\n\n100%|██████████| 630/630 [26:39&lt;00:00,  2.54s/it, loss=0.0255]\n\n\nTrain Loss: 0.0321\nVal Accuracy: 99.83%\nVal Dice Score: 0.9957\n\n\n\n\n\n\n\n\n\nEpoch 5/5\n\n\n100%|██████████| 630/630 [24:39&lt;00:00,  2.35s/it, loss=0.0179]\n\n\nTrain Loss: 0.0209\nVal Accuracy: 99.83%\nVal Dice Score: 0.9959"
  },
  {
    "objectID": "posts/histogram_matching/histogram_matching.html",
    "href": "posts/histogram_matching/histogram_matching.html",
    "title": "Histogram Equalization and Matching",
    "section": "",
    "text": "Code\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nCode\npanda = cv2.imread(\"grey_1.png\",0)\nlenna = cv2.imread(\"grey_2.png\",0)\ncampus = cv2.imread(\"grey_3.png\",0)\nelon = cv2.imread(\"grey_4.png\",0)"
  },
  {
    "objectID": "posts/histogram_matching/histogram_matching.html#creating-the-histogram-of-an-image",
    "href": "posts/histogram_matching/histogram_matching.html#creating-the-histogram-of-an-image",
    "title": "Histogram Equalization and Matching",
    "section": "Creating the Histogram of an Image",
    "text": "Creating the Histogram of an Image\nWe have a discrete grayscale image \\(\\{x\\}\\) with \\(n_i\\) as the number of occurences of gray level \\(i\\). The probability of occurence of a pixel of level \\(i\\) in the image is \\[p_x(i)=p(x=i)=\\frac{n_i}{n}, \\qquad 0\\leq i &lt; L\\] where \\(L\\) is the total number of grey levels in the image (here \\(L=256\\)), \\(n\\) is the total number of pixels in the image (here \\(n = 512 \\times 512\\)), and \\(p_x(i)\\) is the image’s histogram for pixel value \\(i\\) normalized to \\([0,1]\\)\n\n\nCode\ndef imgToHist(img):\n  hist_data = np.zeros((256))\n  # counting number of pixels with a particular value between 0-255\n  for x_pixel in range(img.shape[0]):\n          for y_pixel in range(img.shape[1]):\n              pixel_value = int(img[x_pixel, y_pixel])\n              hist_data[pixel_value] += 1\n\n  # normalizing\n  hist_data/=(img.shape[0]*img.shape[1])\n\n  # returning the hist\n  return hist_data"
  },
  {
    "objectID": "posts/histogram_matching/histogram_matching.html#histogram-equalization",
    "href": "posts/histogram_matching/histogram_matching.html#histogram-equalization",
    "title": "Histogram Equalization and Matching",
    "section": "Histogram Equalization",
    "text": "Histogram Equalization\nWe define the cumulative distribution function corresponsing to \\(i\\) as \\[\\text{cdf}_x(i)=\\sum\\limits_{j=0}^{i}p_x(x=j)\\] which is also the image’s accumulated normalized histogram. \\ We would like to create a transformation of the form \\(y=T(x)\\) to produce a new image \\(\\{y\\}\\), with a flat histogram. Such as image would have a linearized cumulative distribution function across the value range \\[\\text{cdf}_y(i)=(i+1)K, \\qquad 0 \\leq i &lt; L\\] for some constant K. The properties of the CDF allow us to perform such a transform: \\[y = T(k) = \\text{cdf}_x(k)\\] where k is in the range \\([0, L-1]\\). \\(T\\) maps the levels into \\([0,1]\\) since we’re using normalized histogram of \\(\\{x\\}\\). In order to map the map the values into their original range, the following transformation needs to be applied: \\[y' = y \\cdot (\\text{max}\\{x\\} - \\text{min}\\{x\\}) + \\text{min}\\{x\\} = y \\cdot (L-1)\\] \\(y\\) is a real value while \\(y'\\) has to be integer. The mapped value \\(y'\\) should be \\(0\\) for the range of \\(0 &lt; y \\leq \\frac{1}{L}\\). And \\(y' = 1\\) for \\(\\frac{1}{L}&lt;y\\leq \\frac{2}{L}\\), \\(y' = 2\\) for \\(\\frac{2}{L}&lt;y\\leq \\frac{3}{L}\\), … \\(y' = L-1\\) for \\(\\frac{L-1}{L}&lt;y\\leq 1\\). Then the quantization formula for \\(y\\) to \\(y'\\) should be \\[y'=\\text{ceil}(L \\cdot y)-1\\] (\\(y'=-1\\) when \\(y=0\\), however, it does not happen just because \\(y=0\\) means that there is no pixel corresponding to that value.)\n\n\nCode\ndef equalize(img):\n  hist_data = imgToHist(img)\n  cdf_hist = np.cumsum(hist_data) # gives the CDF\n  equalized_hist = np.ceil(cdf_hist * 256) - 1 # maps back into original range\n  enhancedImg = np.zeros_like(img)\n  # now we need to replace initial pixel values with final pixel values\n  for x_pixel in range(img.shape[0]):\n          for y_pixel in range(img.shape[1]):\n              pixel_val = int(img[x_pixel, y_pixel])\n              enhancedImg[x_pixel, y_pixel] = equalized_hist[pixel_val]\n  return enhancedImg\n\n\nA function is defined to plot an images and their histograms before and after equalization. cmap is explicity defined to be gray since the default color map is virdis\n\n\nCode\ndef showEqualizedImg(image, enhanced_image):\n  source_histogram = imgToHist(image)\n  equalized_histogram = imgToHist(enhanced_image)\n  fig, axes = plt.subplots(1, 4, figsize=(16, 4))\n  # source image\n  axes[0].imshow(image, cmap='gray')\n  axes[0].set_title('Source Image')\n  axes[0].axis('off')\n  # source histogram\n  axes[1].bar(np.arange(len(source_histogram)), source_histogram, color = \"Blue\")\n  axes[1].set_title('Source Histogram')\n  # equalized histogram\n  axes[2].bar(np.arange(len(equalized_histogram)), equalized_histogram, color = \"Red\")\n  axes[2].set_title('Equalized Histogram')\n  # enhanced image\n  axes[3].imshow(enhanced_image, cmap='gray')\n  axes[3].set_title('Enhanced Image')\n  axes[3].axis('off')\n\n  plt.tight_layout()\n  plt.show()\n\n\n\n\nCode\nshowEqualizedImg(panda, equalize(panda))\nshowEqualizedImg(lenna, equalize(lenna))\nshowEqualizedImg(campus, equalize(campus))\nshowEqualizedImg(elon, equalize(elon))"
  },
  {
    "objectID": "posts/histogram_matching/histogram_matching.html#histogram-matching",
    "href": "posts/histogram_matching/histogram_matching.html#histogram-matching",
    "title": "Histogram Equalization and Matching",
    "section": "Histogram Matching",
    "text": "Histogram Matching\nConsider a grayscale input image \\(X\\). It has a probability density function \\(p_r(r)\\), where \\(r\\) is a grayscale value, and \\(p_r(r)\\) is the probability of that value. This probability can easily be computed from the histogram of the image by \\[p_r(r_j)=\\frac{n_j}{n}\\] Where \\(n_j\\) is the frequency of the grayscale value \\(r_j\\), and \\(n\\) is the total number of pixels in the image. Now consider a desired output probability density function \\(p_z(z)\\). A transformation of \\(p_r(r)\\) is needed to convert it to \\(p_z(z)\\). \\ Each pdf (probability density function) can be mapped to its cumulative distribution function by \\[S(r_k)=\\sum\\limits_{j=0}^{k}p_r(r_j), \\qquad k =0,1,2,...,L-1\\] \\[G(z_k)=\\sum\\limits_{j=0}^{k}p_z(z_j), \\qquad k =0,1,2,...,L-1\\] Where L is the total number of gray levels (here \\(L=256\\)). \\ The idea is to map each \\(r\\) value in \\(X\\) to the \\(z\\) value that has the same probability in the desired pdf: \\[S(r_j) = G(z_i) \\text{ or } z = G^{-1}(S(r))\\]\nAlgorithm \\ Given two images, the reference and the target images, we compute their histograms. Following, we calculate the cumulative distribution functions of the two images’ histograms: \\(F_1\\) for for the reference image and \\(F_2\\) for the target image. Then for each gray level \\(G_1 \\in [0, 255]\\), we find the gray level \\(G_2\\) for which \\(F_1(G_1)=F_2(G_2)\\) and this is the result of histogram matching function: \\(M(G_1) = G_2\\). Finally, we apply the function \\(M\\) on each pixel of the reference image.\n\n\nCode\ndef match(source_image, target_image):\n  source_hist = imgToHist(source_image)\n  target_hist = imgToHist(target_image)\n  cdf_source_hist = np.cumsum(source_hist) # F1\n  cdf_target_hist = np.cumsum(target_hist) # F2\n  matched = np.zeros_like(source_image)\n  for x_pixel in range(source_image.shape[0]):\n    for y_pixel in range(source_image.shape[1]):\n      pixel_val = source_image[x_pixel, y_pixel] # G1\n      index = 255 # assume G2\n      # to find G2\n      for i in range(256):\n        if cdf_target_hist[i] - cdf_source_hist[pixel_val] &gt;= 0:\n          index = i\n          break\n      # applying function on each pixel\n      matched[x_pixel, y_pixel] = index\n  return matched\n\n\nA function is defined to source image, target image and image obtained upon matching. \\ cmap is explicity defined to be gray since the default color map is virdis\n\n\nCode\ndef showMatchedImg(source_img, target_img, matched_img):\n    fig, axes = plt.subplots(1, 3, figsize=(12, 4))\n    # source image\n    axes[0].imshow(source_img, cmap='gray')\n    axes[0].set_title('Source Image')\n    axes[0].axis('off')\n    # target image\n    axes[1].imshow(target_img, cmap='gray')\n    axes[1].set_title('Target Image')\n    axes[1].axis('off')\n    # matched image\n    axes[2].imshow(matched_img, cmap='gray')\n    axes[2].set_title('Matched Image')\n    axes[2].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\ndef imageMatch(sourceImage, targetImage):\n  matchedImage = match(sourceImage, targetImage)\n  showMatchedImg(sourceImage, targetImage, matchedImage)\n# according to submission requirements\nimageMatch(panda, lenna)\nimageMatch(elon, campus)\nimageMatch(lenna, elon)\nimageMatch(campus, panda)"
  },
  {
    "objectID": "posts/histogram_matching/histogram_matching.html#references",
    "href": "posts/histogram_matching/histogram_matching.html#references",
    "title": "Histogram Equalization and Matching",
    "section": "References",
    "text": "References\n[1] Wikipedia Contributors, “Histogram equalization,” Wikipedia, May 18, 2018. https://en.wikipedia.org/wiki/Histogram_equalization\n[2] “Histogram matching,” Wikipedia, Feb. 07, 2022. https://en.wikipedia.org/wiki/Histogram_matching"
  },
  {
    "objectID": "posts/matrix_factorization/matrix_factorization.html",
    "href": "posts/matrix_factorization/matrix_factorization.html",
    "title": "Image Reconstruction using Matrix Factorization",
    "section": "",
    "text": "general discussion on matrix factorization lorem ipsum dolor sit amet\nWe begin by setting up imports.\nCode\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torchmetrics.functional.image import peak_signal_noise_ratio\nfrom torchmetrics.functional.regression import mean_squared_error\ndevice = torch.device('mps' if torch.backends.mps.is_available() else 'cpu')\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\n\nfrom sklearn.kernel_approximation import RBFSampler"
  },
  {
    "objectID": "posts/matrix_factorization/matrix_factorization.html#setting-up-our-image",
    "href": "posts/matrix_factorization/matrix_factorization.html#setting-up-our-image",
    "title": "Image Reconstruction using Matrix Factorization",
    "section": "Setting up our image",
    "text": "Setting up our image\nLet’s say we have a complete input image for testing purposes. We will first mask it by putting in nan values randomly. We’ll use the masked image to reconstruct the original image using matrix factorization.\nSince our image will be in RGB, we’ll perform all operations for the three channels seperately.\n\n\nCode\n# function to mask the image\ndef mask_image(img,\n               prop,\n               device):\n    img_copy = img.clone().to(device)\n    mask = torch.rand(img.shape[1:]) &lt; prop # uniform distribution\n    img_copy[0][mask] = float('nan')\n    img_copy[1][mask] = float('nan')\n    img_copy[2][mask] = float('nan')\n    return img_copy, mask\n\n# function that randomly removes 900 pixels from the image\ndef random_mask_image(img, device):\n    img_copy = img.clone().to(device)\n    h, w = img.shape[1], img.shape[2]\n    total_pixels = h * w\n    \n    # Randomly select 900 pixel indices\n    random_indices = torch.randperm(total_pixels)[:900]\n    \n    # Convert flat indices back to 2D indices (for height and width)\n    mask = torch.zeros(h, w, dtype=torch.bool, device=device)\n    mask.view(-1)[random_indices] = True\n    \n    # Apply NaN mask to each channel\n    img_copy[0][mask] = float('nan')\n    img_copy[1][mask] = float('nan')\n    img_copy[2][mask] = float('nan')\n    \n    return img_copy, mask"
  },
  {
    "objectID": "posts/matrix_factorization/matrix_factorization.html#implementing-matrix-factorization",
    "href": "posts/matrix_factorization/matrix_factorization.html#implementing-matrix-factorization",
    "title": "Image Reconstruction using Matrix Factorization",
    "section": "Implementing Matrix Factorization",
    "text": "Implementing Matrix Factorization\nTo factorize any matrix \\(A\\), we essentially want to learn two matrices \\(W\\) and \\(H\\) such that \\[A=W\\cdot H\\] We want that \\[W, H = argmin_{W',H'}  ||A-W'\\cdot H'||_F^2\\] This can achieved using two methods: gradient descent and alternating least squares.\nLet us first implement the factorize(A, k, device) function using gradient descent. This function will take as input the matrix to be factorized \\(A_{m \\times n}\\), the rank \\(k\\) of decomposition and the device for pytorch code.\nWe first randomly initialize \\(W_{m\\times k}\\) and \\(H_{k\\times n}\\). Then for each iteration of gradient descent we follow the following update rules: \\[W = W - \\alpha \\frac{\\partial ||A-W\\cdot H||_F^2}{\\partial W}\\] \\[H = H - \\alpha \\frac{\\partial ||A-W\\cdot H||_F^2}{\\partial H}\\]\nWe use the torch.optim.Adam optimizer with a learning rate of 0.01.\nSince the input matrix can have certain pixels as nan we use a mask that ensures we only calculate loss by taking the norm of the non-nan values.\n\n\nCode\ndef factorize(A,\n              k,\n              device):\n    \"\"\"Factorize the matrix A into W and H\"\"\"\n    A = A.to(device)\n    # Randomly initialize W, H\n    W = torch.randn(A.shape[0], k, requires_grad=True, device=device)\n    H = torch.randn(k, A.shape[1], requires_grad=True, device=device)\n    # Optimizer\n    optimizer = torch.optim.Adam([W, H], lr=0.01)\n    mask = ~torch.isnan(A)\n    # Train the model\n    for i in range(1000):\n        # Compute the loss\n        diff_matrix = torch.mm(W, H) - A\n        diff_vector = diff_matrix[mask] # makes a 1D tensor\n        loss = torch.norm(diff_vector)\n        \n        # Zero the gradients\n        optimizer.zero_grad()\n        \n        # Backpropagate\n        loss.backward()\n        \n        # Update the parameters\n        optimizer.step()\n        \n    return W, H, loss\n\n\n\n\nCode\ndef extract_not_nan_coordinates_pixels(image, device): \n  channels, height, width = image.shape\n  coords = []\n  pixel_values = [] \n\n  for y in range(height):\n    for x in range(width):\n      if image[0][x][y].isnan():\n        continue\n      coords.append([x, y])\n      pixel_values.append(image[:, x, y].tolist())\n\n  coords = torch.tensor(coords, dtype=torch.float32)\n  pixel_values = torch.tensor(pixel_values, dtype=torch.float32)\n\n  return coords.to(device), pixel_values.to(device)\n\ndef create_linear_model(input_dim, output_dim, device):\n  return nn.Linear(input_dim, output_dim).to(device)\n\ndef train(coords, pixels, model, learning_rate=0.01, epochs=1000, threshold=1e-6, verbose=True):\n\n    criterion = nn.MSELoss() # define the loss function (mse)\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate) # use the adam optimizer with the specified learning rate\n    previous_loss = float('inf') # initialize w very large value (for early stopping)\n\n    # training loops\n    for epoch in range(epochs):\n        optimizer.zero_grad() # reset the gradient of the optimizer\n        outputs = model(coords) # compute the output\n        loss = criterion(outputs, pixels) # calculate the loss that we defined earlier\n        loss.backward() # compute teh gradients of the loss with respect to the parameters\n        optimizer.step() # update the parameters based on the gradients computed above\n\n        # check for early stopping\n        if abs(previous_loss - loss.item()) &lt; threshold:\n            print(f\"Stopping early at epoch {epoch} with loss: {loss.item():.6f}\")\n            break\n\n        previous_loss = loss.item() # update the previous loss\n\n        if verbose and epoch % 100 == 0:\n            print(f\"Epoch {epoch} loss: {loss.item():.6f}\")\n\n    return loss.item()\n\ndef create_rff_features(tensor, num_features, sigma, device):\n    rff = RBFSampler(n_components=num_features, gamma=1/(2 * sigma**2), random_state=42)\n    tensor = torch.tensor(rff.fit_transform(tensor.cpu().numpy())).float().to(device)\n    return tensor\n\ndef create_coordinate_map(height, width, device, inv=False): # given the height and width of an image this function creates a coordinate map and returns it\n  coords = []\n  if not inv:\n    for x in range(height):\n      for y in range(width):\n        coords.append([x, y])\n  else:\n     for y in range(height):\n      for x in range(width):\n        coords.append([x, y])\n\n  return torch.tensor(coords, dtype=torch.float32).to(device)"
  },
  {
    "objectID": "posts/matrix_factorization/matrix_factorization.html#defining-metrics-for-the-problem",
    "href": "posts/matrix_factorization/matrix_factorization.html#defining-metrics-for-the-problem",
    "title": "Image Reconstruction using Matrix Factorization",
    "section": "Defining metrics for the problem",
    "text": "Defining metrics for the problem\nWe use peak_signal_noise_ratio and mean_squared_error from the torchmetrics library as metrics to evaluate the performance of our model. Peak Signal to Noise ratio: Consider we have single channel of an image \\(A_{m \\times n}\\) and its learned factorization \\(A'=W_{m \\times k}H_{k \\times n}\\), MSE is defined as \\[MSE = \\frac{1}{mn}\\sum_{i=0}^{m-1} \\sum_{j=0}^{n-1} \\left(A(i,j)-A'(i,j)\\right)^2\\] The PSNR (in dB) is defined as \\[PSNR = 10\\log_{10}\\left(\\frac{MAX_A^2}{MSE}\\right) = 20\\log_{10}\\left(MAX_A\\right)-10\\log_{10}\\left(MSE\\right)\\] Here \\(MAX_A\\) is the maximum possible pixel value of the given channel \\(A\\) of an iamge. For our case \\(MAX_A\\) is simply 1.\n\n\nCode\n# function to compute the RMSE and PSNR\ndef metrics(img, reconstructed_img):\n    rmse = mean_squared_error(target = img.reshape(-1),\n                             preds=reconstructed_img.reshape(-1),\n                             squared=False)\n    psnr = peak_signal_noise_ratio(target=img.reshape(-1),\n                                   preds=reconstructed_img.reshape(-1))\n    return rmse, psnr"
  },
  {
    "objectID": "posts/matrix_factorization/matrix_factorization.html#testing",
    "href": "posts/matrix_factorization/matrix_factorization.html#testing",
    "title": "Image Reconstruction using Matrix Factorization",
    "section": "Testing",
    "text": "Testing\nWe now write a function plot_image_completion() that takes as input: \n\nimg: the original image\n\n\nprop_list: a list of proportions of pixels to mask\n\n\nfactors_list: a list of the decomposition factors\n\n\ndevice: PyTorch device\n\nNow for each proportion of image to be masked prop in prop_list, we first mask the image, then for each r in factors_list we reconstruct all three channels and stack them to get a complete reconstructed image.\nWe also keep a track of RMSE and PSNR metrics for plotting later on.\n\n\nCode\ndef plot_image_completion(img,\n                          prop_list,\n                          factors_list,\n                          device): \n\n    fig = plt.figure(figsize=(len(factors_list) * 4, len(prop_list) * 18))\n    gs = GridSpec(3*len(prop_list)+1, len(factors_list), figure=fig)\n    \n    curr_row = 0\n    for i, prop in enumerate(prop_list):\n        masked_img, mask = mask_image(img=img,\n                                  prop=prop,\n                                  device=device)\n        \n        axorg = fig.add_subplot(gs[curr_row, 0:len(factors_list)//2])\n        axmask = fig.add_subplot(gs[curr_row, len(factors_list)//2:])\n        curr_row += 1\n\n        axorg.imshow(img.permute(1,2,0).cpu().numpy())\n        axorg.set_title(f\"Original image\")\n\n        axmask.imshow(masked_img.permute(1,2,0).cpu().numpy())\n        axmask.set_title(f\"Masked image with prop={prop}\")\n\n        rmses = []\n        psnrs = []\n        for j, r in enumerate(factors_list):\n            Wr, Hr, lossr = factorize(masked_img[0], r, device=device)\n            Wg, Hg, lossg = factorize(masked_img[1], r, device=device)\n            Wb, Hb, lossb = factorize(masked_img[2], r, device=device)\n            reconstructed_img = torch.clamp(torch.stack([torch.mm(Wr, Hr).detach(), torch.mm(Wg, Hg).detach(), torch.mm(Wb, Hb).detach()], dim=0), 0, 1)\n\n            rmse, psnr = metrics(img, reconstructed_img)\n        \n            rmses.append(rmse.cpu().numpy())\n            psnrs.append(psnr.cpu().numpy())\n\n            # Plot reconstructed image with metrics in the loop\n            ax = fig.add_subplot(gs[curr_row, j])\n            ax.imshow(reconstructed_img.permute(1,2,0).cpu().numpy())\n            ax.set_title(f\"prop= {prop}, r={r}, RMSE={rmse:.4f}, PSNR={psnr:.4f}\")\n            \n        curr_row += 1\n        # Plot RMSE and PSNR vs r outside the loop\n        axrmse = fig.add_subplot(gs[curr_row,0:len(factors_list)//2])\n        axrmse.plot(factors_list, rmses, marker='o')\n        axrmse.set_xlabel('r')\n        axrmse.set_ylabel('RMSE')\n        axrmse.set_title('RMSE vs r')\n\n        axpsnr = fig.add_subplot(gs[curr_row,len(factors_list)//2:])\n        axpsnr.plot(factors_list, psnrs, marker='o')\n        axpsnr.set_xlabel('r')\n        axpsnr.set_ylabel('PSNR')\n        axpsnr.set_title('PSNR vs r')\n\n        curr_row += 1\n\n    # Make the layout tight for better appearance\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\nkrustykrab = torchvision.io.read_image(\"images/krustykrab.png\")\nkrustykrab = krustykrab.to(dtype=torch.float32, device=device)/255\n\ntransform = torchvision.transforms.CenterCrop((500, 500))\n\nkrustykrab = transform(krustykrab)\n\nplt.figure(figsize=(6, 4))  \nplt.imshow(krustykrab.permute(1,2,0).cpu().numpy())\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nCode\nplot_image_completion(krustykrab, prop_list=[0.1, 0.4, 0.8], factors_list=[50,100,300,500], device=device)"
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html",
    "href": "posts/otsu_thresholding/otsu_thresholding.html",
    "title": "Otsu’s Threshold Algorithm",
    "section": "",
    "text": "The Otsu method is used to find the threshold value for a grayscale image. The threshold value is used to separate the image into two parts: the background and the foreground. The Otsu method is based on the assumption that the image has two classes of pixels: the background and the foreground. The method calculates the threshold value that minimizes the intra-class variance and maximizes the inter-class variance. The threshold value is used to separate the image into two parts: the background and the foreground. The Otsu method is widely used in image processing and computer vision applications.\nCode\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nCode\nbookpage_1 = cv2.imread(\"bookpage_1.jpeg\",0)\nbookpage_2 = cv2.imread(\"bookpage_2.jpeg\",0)\npanther = cv2.imread(\"panther.jpeg\",0)\ntom = cv2.imread(\"tom.jpeg\",0)"
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html#creating-the-histogram-of-an-image",
    "href": "posts/otsu_thresholding/otsu_thresholding.html#creating-the-histogram-of-an-image",
    "title": "Otsu’s Threshold Algorithm",
    "section": "Creating the Histogram of an Image",
    "text": "Creating the Histogram of an Image\nWe have a discrete grayscale image \\(\\{x\\}\\) with \\(n_i\\) as the number of occurences of gray level \\(i\\). The probability of occurence of a pixel of level \\(i\\) in the image is \\[p_x(i)=p(x=i)=\\frac{n_i}{n}, \\qquad 0\\leq i &lt; L\\] where \\(L\\) is the total number of grey levels in the image (here \\(L=256\\)), \\(n\\) is the total number of pixels in the image, and \\(p_x(i)\\) is the image’s histogram for pixel value \\(i\\) normalized to \\([0,1]\\)\n\n\nCode\ndef imgToHist(img):\n  hist_data = np.zeros((256))\n  # counting number of pixels with a particular value between 0-255\n  for x_pixel in range(img.shape[0]):\n          for y_pixel in range(img.shape[1]):\n              pixel_value = int(img[x_pixel, y_pixel])\n              hist_data[pixel_value] += 1\n\n  # normalizing\n  hist_data/=(img.shape[0]*img.shape[1])\n\n  # returning the hist\n  return hist_data"
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html#implementing-otsus-algorithm",
    "href": "posts/otsu_thresholding/otsu_thresholding.html#implementing-otsus-algorithm",
    "title": "Otsu’s Threshold Algorithm",
    "section": "Implementing Otsu’s Algorithm",
    "text": "Implementing Otsu’s Algorithm\nThe algorithm exhaustively searches for the threshold that minimizes the intra-class variance, defined as a weighted sum of variances of the two classes: \\[\\sigma_w^2 (t) = w_0(t)\\sigma_0^2 (t)+w_1(t)\\sigma_1^2 (t)\\] Weights \\(w_0\\) and \\(w_1\\) are the probabilities of the two classes separated by a threshold \\(t\\), and \\(\\sigma_0^2\\) and \\(\\sigma_1^2\\) are variance of these two classes. The class probability \\(w_{\\{0,1\\}}(t)\\) is computed from the \\(L\\) bins of the histogram: \\[w_0(t) = \\sum\\limits_{i=0}^{t-1}p(i)\\] \\[w_1(t) = \\sum\\limits_{i=t}^{L-1}p(i)\\] For two classes, minimized the intra-class variance is equivalent to maximizing inter-class variance: \\[\\sigma_b^2(t) = \\sigma^2 -\\sigma_w^2(t) = w_0(t)(\\mu_0-\\mu_T)^2 + w_1(t)(\\mu_1-\\mu_T)^2 = w_0(t)w_1(t)[\\mu_0(t)-\\mu_1(t)]^2\\] which is expressed in terms of class probabilities \\(w\\) and class means \\(\\mu\\) where the class means \\(\\mu_0(t)\\), \\(\\mu_1(t), \\mu_T\\) are: \\[\\mu_0(t) = \\frac{\\sum\\limits_{i=0}^{t-1}ip(i)}{w_0(t)}\\] \\[\\mu_1(t) = \\frac{\\sum\\limits_{i=t}^{L-1}ip(i)}{w_1(t)}\\] \\[\\mu_T = \\sum\\limits_{i=0}^{L-1}ip(i)\\]\n\nAlgorithm\n\nCompute histogram and probabilites of each intensity level\nSet up initial w_i(0) and u_i(0)\nStep through all possible thresholds \\(t=1,...,\\text{maximum intensity}\\)\nUpdate \\(w_i\\) and \\(\\mu_i\\)\nCompute \\(\\sigma_b^2(t)\\)\nDesired threshold corresponds to maximum \\(\\sigma_b^2(t)\\)\n\n\n\nCode\ndef otsu(img):\n  hist = imgToHist(img)\n  inter_class_variances = []\n  for i in range(256):\n    w0 = sum(hist[0:i])\n    w1 = sum(hist[i:])\n    u0 = sum([x*hist[x]/w0 for x in range(0,i)])\n    u1 = sum([x*hist[x]/w1 for x in range(i,256)])\n    sigma0 = sum([((x-u0)**2)*hist[x] for x in range(0,i)])\n    sigma1 = sum([((x-u1)**2)*hist[x] for x in range(i,256)])\n    sigmaw = w0*sigma0 + w1*sigma1\n    sigmab=w0*w1*(u0-u1)**2\n    inter_class_variances.append(sigmab)\n  # maximizing inter-class variance\n  threshold = inter_class_variances.index(max(inter_class_variances))\n  binarized_image = np.zeros_like(img)\n  for x_pixel in range(img.shape[0]):\n          for y_pixel in range(img.shape[1]):\n              pixel_value = int(img[x_pixel, y_pixel])\n              if pixel_value &gt; threshold:\n                binarized_image[x_pixel, y_pixel] = 255\n              else:\n                binarized_image[x_pixel, y_pixel] = 0\n  return threshold, binarized_image"
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html#binarizing-images-and-plotting-them",
    "href": "posts/otsu_thresholding/otsu_thresholding.html#binarizing-images-and-plotting-them",
    "title": "Otsu’s Threshold Algorithm",
    "section": "Binarizing Images and Plotting Them",
    "text": "Binarizing Images and Plotting Them\n\n\nCode\ndef showBinarizedImg(source_img):\n    fig, axes = plt.subplots(1, 2, figsize=(6, 4))\n    # source image\n    axes[0].imshow(source_img, cmap='gray')\n    axes[0].set_title(r'Source Image')\n    axes[0].axis('off')\n    \n    # binarized image\n    threshold, binarized_img = otsu(source_img)\n    axes[1].imshow(binarized_img, cmap='gray')\n    axes[1].set_title(r\"Binarized Image\")\n    axes[1].axis('off')\n\n    # figure title based on threshold with LaTeX formatting\n    fig.suptitle(fr\"Otsu's Threshold: ${threshold}$\", fontsize=12)\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\nshowBinarizedImg(bookpage_1)\nshowBinarizedImg(bookpage_2)\nshowBinarizedImg(panther)\nshowBinarizedImg(tom)"
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html#adding-gaussian-noise-and-applying-otsus-algorithm",
    "href": "posts/otsu_thresholding/otsu_thresholding.html#adding-gaussian-noise-and-applying-otsus-algorithm",
    "title": "Otsu’s Threshold Algorithm",
    "section": "Adding Gaussian Noise and Applying Otsu’s Algorithm",
    "text": "Adding Gaussian Noise and Applying Otsu’s Algorithm\nBy adding noise to an image, we can simulate real-world scenarios where images are often corrupted by various noise sources (e.g., sensor noise, transmission errors). This allows us to evaluate how well Otsu’s method performs under different noise conditions and its robustness to noise.\n\n\nCode\ndef addGaussianNoise(img, variance):\n  mean = 0.0\n  std = np.sqrt(variance)\n  noisy_img = img + np.random.normal(mean, std, img.shape)\n  noisy_img_clipped = np.clip(noisy_img, 0, 255 )\n  return noisy_img_clipped\n\n\n\n\nCode\ndef showBinarizedImgWithNoise(source_img, variance):\n    threshold, binarized_img = otsu(source_img)\n    noisy_source_img = addGaussianNoise(source_img, variance)\n    noisy_threshold, binarized_noisy_img = otsu(noisy_source_img)\n    \n    fig, axes = plt.subplots(1, 4, figsize=(12, 4))\n\n    # Source and Binarized Image Titles (without noise)\n    source_title = r\"Source Image\"\n    binary_title = fr\"Binarized Image (Threshold: ${threshold}$)\"\n\n    # Noisy Source and Binarized Image Titles (with noise)\n    noisy_source_title = fr\"Source Image with Noise ($\\sigma^2={variance}$)\"\n    noisy_binary_title = fr\"Binarized Image (Threshold: ${noisy_threshold}$)\"\n\n    # Source image\n    axes[0].imshow(source_img, cmap='gray')\n    axes[0].set_title(source_title)\n    axes[0].axis('off')\n\n    # Binarized image\n    axes[1].imshow(binarized_img, cmap='gray')\n    axes[1].set_title(binary_title)\n    axes[1].axis('off')\n\n    # Source image with noise\n    axes[2].imshow(noisy_source_img, cmap='gray')\n    axes[2].set_title(noisy_source_title)\n    axes[2].axis('off')\n\n    # Binarized image with noise\n    axes[3].imshow(binarized_noisy_img, cmap='gray')\n    axes[3].set_title(noisy_binary_title)\n    axes[3].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n\n\n\n\nCode\nvar = 1000\nshowBinarizedImgWithNoise(bookpage_1, var)\nshowBinarizedImgWithNoise(bookpage_2, var)\nshowBinarizedImgWithNoise(panther, var)\nshowBinarizedImgWithNoise(tom, var)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThe threshold value obtained from Otsu’s method may vary significantly depending on the noise level. Higher noise levels can lead to less accurate thresholding.\nThe quality of the binarized image can degrade with increasing noise. Noisy pixels can be incorrectly classified as foreground or background, leading to a loss of detail and accuracy.\nBy analyzing the results under different noise levels, we can assess the robustness of Otsu’s method. Some variations of Otsu’s method, such as adaptive thresholding, may be more resilient to noise.\nWe might consider pre-processing techniques like noise reduction filters to improve the input image quality before applying Otsu’s method."
  },
  {
    "objectID": "posts/otsu_thresholding/otsu_thresholding.html#references",
    "href": "posts/otsu_thresholding/otsu_thresholding.html#references",
    "title": "Otsu’s Threshold Algorithm",
    "section": "References",
    "text": "References\nWikipedia Contributors, “Otsu’s method,” Wikipedia, Aug. 16, 2020. https://en.wikipedia.org/wiki/Otsu%27s_method\n\n\nCode\nintervals = [(1,4), (4, 7)]\ni = 0\nwhile i!=len(intervals)-1:\n    if intervals[i][1] &gt; intervals[i+1][0]:\n        intervals = intervals[:i]+[(intervals[i][0], intervals[i+1][1])]+intervals[i+2:]\n    else:\n        i+=1\nintervals\n\n\n[(1, 4), (4, 7)]"
  }
]